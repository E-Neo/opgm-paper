\section{Evaluation}\label{sec:experiments}
\subsection{Setup}
\subsubsection{Environment}
The experiments were performed on a single machine with two Intel Xeon Processor E5--2699 v4 CPUs, 64 GB of RAM,
and an 800GB SSD\@.
The machine has 22 cores and 44 threads.
Memory caches were dropped before each experiment to force disk loads.
\subsubsection{Datasets and Queries}
Our datasets and the queries have 2 vertex labels and 2 edge labels.
We use real-world data graphs~\cite{snapnets} listed in Table~\ref{tab:datasets} with different sizes. The vertices and edges in the datasets are labeled randomly as in~\cite{DBLP:journals/pvldb/MhedhbiS19}.
The queries (Figure~\ref{img:queries}) are selected from existing work~\cite{DBLP:conf/cloud/SerafiniMS17,DBLP:journals/pvldb/MhedhbiS19}.
The labels of vertices and edges are tagged randomly,
and they are represented by the colors and patterns of lines in Figure~\ref{img:queries}.
The queries we choose represent different topologies: trees, chains, and cyclic graphs.
$q_9$ --- $q_{12}$ are queries with multi-edges.
\begin{table}
  \caption{Datasets}\label{tab:datasets}
  \begin{tabular}{lrr}
    \toprule
    Name & $|V|$ & $|E|$ \\
    \midrule
    soc-Epinions (EP) & 76K & 509K \\
    web-Google (GO) & 876K & 5.1M \\
    web-BerkStan (BS) & 685K & 7.6M \\
    soc-LiveJournal (LJ) & 4.8M & 69M \\
    com-Orkut (OK) & 3.1M & 117.2M \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\textwidth]{img/queries.pdf}
  \caption{The queries.}\label{img:queries}
\end{figure}

\subsection{Preprocessing Cost}
We first study the preprocessing cost of SeqStar's vertex-centric storage engine.
The preprocessing incurs an external sort on the original graph data to generate the formatted data graph shown in Figure~\ref{img:data_graph}.
The cost is $\mathcal{O}(n \log n)$ where $n$ is the size of the unsorted graph data.
We use 32-bit integers to store the vertex IDs, and 16-bit integers to store the vertex/edge labels.
As is shown in Figure~\ref{img:exp_preprocessing}, the preprocessing time grows linearly with respect to the size of graph data.
In fact, the preprocessing time of the vertex-centric storage is significantly smaller than the execution time of complex queries (\S\ref{sec:experiments_compare}).

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.4\textwidth]{img/exp_preprocessing.pdf}
  \caption{Preprocessing time of SeqStar.}\label{img:exp_preprocessing}
\end{figure}
\subsection{Comparative Performance}\label{sec:experiments_compare}
We evaluate the performance of SeqStar, Graphflow and Neo4j (Version 4.2.3).
Graphflow is the state-of-the-art in-memory subgraph querying system,
and it is the fastest baseline we are aware of.
Graphflow is a JVM based system, and we set the maximum size of the JVM heap to 60GB to full use of the main memory.
However Graphflow does not support queries with multi-edges, i.e., $q_9$ --- $q_{12}$,
and it will report fake results for these queries\footnote{Graphflow will report matchings even the data does not contain multi-edges.}.
Therefore we use $q_1$ --- $q_8$ to compare to Graphflow.
Neo4j is a widely used industrial graph database, which fully supports the property graph model.
However, Neo4j runs much slower than Graphflow and SeqStar for graph matching.
Therefore we only test $q_9$ --- $q_{12}$  on Neo4j that Graphflow doesn't support.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{img/exp_compare.pdf}
  \caption{Execution time of SeqStar and Graphflow.}\label{img:exp_compare}
\end{figure*}

Figure~\ref{img:exp_compare} compares SeqStar against Graphflow.
For datasets EP, GO and BS, SeqStar outperforms the existing work by $1.4\times$ to $26\times$.
Graphflow uses edges as the basic matching unit and applies worst case optimal join on the edges.
In contrast, our SeqStar use stars to filter the graph data.
As stars contain more filtering constraints than edges, SeqStar results in smaller intermediate results.
Moreover, SeqStar compresses the intermediate results and performs parallel pipeline join on the compressed data.
Therefore, the memory usage of SeqStar is much smaller than Graphflow,
and the time spent on processing the intermediate data is reduced.
%% We notice that the CPU utilization of Graphflow drops quickly as some cores finish their work early,
%% leading to the long tail problem.
%% In contrast, SeqStar overcomes the long tail problem by work stealing and our SeqStar is the clear winner.

For datasets LJ and OK, the datasets are so large that the disk scanning overhead of SeqStar predominates as the datasets become larger.
Therefore Graphflow outperforms SeqStar on $q_1$, $q_2$, $q_3$ and $q_7$.
However, for complex query such as $q_6$, SeqStar still outperforms Graphflow in dataset OK.
Moreover, Graphflow fails to process $q_4$, $q_6$ (except dataset OK) and $q_8$ due to the out of memory problem.
Whereas our SeqStar still works on these queries.
Both SeqStar and Graphflow fail to finish enumerating $q_5$ in 35 minutes as the result size is quite large:
There are $3.2 \times 10^{13}$ rows for LJ and $3.8 \times 10^{14}$ rows for OK\@.
However, our SeqStar is able to analyze the compressed data and report useful information,
e.g., the result size for LJ and OK on $q_5$ can be reported in less than 20 seconds.

Figure~\ref{img:exp_compare_neo4j} shows the execution time of SeqStar and Neo4j on query $q_9$ to $q_{12}$.
The dataset OK does not contain multi-edges and we omit it in this experiment.
Neo4j fails to get the final answer of $q_{11}$, $q_{12}$ on BS, and $q_{10}$ --- $q_{12}$ on LJ in 35min.
Our SeqStar outperforms Neo4j by orders of magnitude.
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.45\textwidth]{img/exp_compare_neo4j.pdf}
  \caption{Execution time of SeqStar and Neo4j.}\label{img:exp_compare_neo4j}
\end{figure}
\subsection{Compression Ratio}\label{sec:experiments_compress}
Table~\ref{tab:compressed} shows the size of compressed intermediate results (SuperRows).
The size of the SuperRows (include metadata) is much smaller (less than $22\%$) compared to the size of the original graph data (Figure~\ref{img:exp_preprocessing}).
SeqStar will only generate intermediate results from star matching,
and they can be cached in memory to boost queries.

\begin{table}
  \caption{Size of compressed intermediate results. (MB)}\label{tab:compressed}
  \begin{tabular}{lrrrrr}
    \toprule
    $q$ &  EP &  GO &  BS &  LJ &  OK \\
    \midrule
    1  &   1 &  15 &  19 & 174 & 267 \\
    2  &   2 &  16 &  21 & 208 & 317 \\
    3  &   2 &  15 &  19 & 238 & 366 \\
    4  &   1 &  16 &  20 & 206 & 314 \\
    5  &   1 &   9 &  11 & 153 & 240 \\
    6  &   3 &  29 &  37 & 411 & 619 \\
    7  &   4 &  33 &  43 & 477 & 749 \\
    8  &   3 &  25 &  31 & 343 & 505 \\
    9  &   1 &   6 &   8 & 119 & 0 \\
    10 &   2 &  13 &  16 & 244 & 0 \\
    11 &   2 &  14 &  18 & 249 & 0 \\
    12 &   2 &  11 &  14 & 251 & 0 \\
    \bottomrule
  \end{tabular}
\end{table}

Table~\ref{tab:compression_ratio} shows the compression ratio of SeqStar's SuperRow.
$93\%$ of the data have a compression ratio larger than $10$;
$73\%$ larger than $100$; and $59\%$ larger than $1000$.

We also studied the impact of metadata in SuperRows (Figure~\ref{img:compress}),
and find that the metadata takes no more than $35\%$ of the space.
Over $68\%$ of the cases, the metadata use less than $25\%$ of the space.

\begin{table}
  \caption{The compression ratio of SuperRows.}\label{tab:compression_ratio}
  \begin{tabular}{lrrrrr}
    \toprule
    $q$ &     EP &       GO &          BS &       LJ &        OK \\
    \midrule
    1  &     26 &       35 &         699 &       37 &        90 \\
    2  &   2225 &       15 &         114 &     9227 &    174687 \\
    3  &   1603 &      207 &        5645 &     3373 &      6823 \\
    4  &    812 &       13 &         114 &     2154 &      6666 \\
    5  & 308241 &      632 &        5144 &  3966263 &  30030074 \\
    6  &  59045 &     1091 &       28381 &   353610 &   1031953 \\
    7  & 550049 & 17468015 & 35906569518 & 33560087 & 415665030 \\
    8  &     24 &        4 &          10 &       25 &        31 \\
    9  &     11 &        2 &           7 &       14 &        \\
    10 &   3397 &    37339 &     8700505 &    13089 &        \\
    11 &  10209 &      331 &       14216 &    38987 &        \\
    12 &  11163 &      244 &       13311 &    89171 &        \\
    \bottomrule
  \end{tabular}
\end{table}

\input{sections/experiments_continued}
